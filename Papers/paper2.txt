

23

adaptive sequential recommendation for discussion
forums on moocs using context trees
fei mi
boi faltings
artificial intelligence lab
école polytechnique fédérale de lausanne, switzerland
firstname.lastname@epfl.ch
abstract

massive open online courses (moocs) have demonstrated growing popularity and rapid development in recent years. discussion
forums have become crucial components for students and instructors to widely exchange ideas and propagate knowledge. it is important to recommend helpful information from forums to students
for the benefit of the learning process. however, students or instructors update discussion forums very often, and the student preferences over forum contents shift rapidly as a mooc progresses.
so, mooc forum recommendations need to be adaptive to these
evolving forum contents and drifting student interests. these frequent changes pose a challenge to most standard recommendation
methods as they have difficulty adapting to new and drifting observations. we formalize the discussion forum recommendation
problem as a sequence prediction problem. then we compare different methods, including a new method called context tree (ct),
which can be effectively applied to online sequential recommendation tasks. the results show that the ct recommender performs
better than other methods for moocs forum recommendation task.
we analyze the reasons for this and demonstrate that it is because
of better adaptation to changes in the domain. this highlights the
importance of considering the adaptation aspect when building recommender system with drifting preferences, as well as using machine learning in general.

keywords

moocs forum recommendation, context tree, model adaptation

1. introduction

with the increased availability of data, machine learning has become the method of choice for knowledge acquisition in intelligent
systems and various applications. however, data and the knowledge derived from it have a timeliness, such that in a dynamic environment not all the knowledge acquired in the past remains valid.
therefore, machine learning models should acquire new knowledge incrementally and adapt to the dynamic environments. today, many intelligent systems deal with dynamic environments: information on websites, social networks, and applications in com-

mercial markets. in such evolving environments, knowledge needs
to adapt to the changes very frequently. many statistical machine
learning techniques interpolate between input data and thus their
models can adapt only slowly to new situations. in this paper,
we consider the dynamic environments for recommendation task.
drifting user interests and preferences [3, 11] are important in building personal assistance systems, such as recommendation systems
for social networks or for news websites where recommendations
need be adaptive to drifting trends rather than recommending obsolete or well-known information. we focus on the application
of recommending forum contents for massive open online courses
(moocs) where we found that the adaptation issue is a crucial aspect for providing useful and trendy information to students.
the rapid emergence of some mooc platforms and many moocs
provided on them has opened up a new era of education by pushing
the boundaries of education to the general public. in this special online classroom setting, sharing with your classmates or asking help
from instructors is not as easy as in traditional brick-and-mortar
classrooms. so discussion forums there have become one of the
most important components for students to widely exchange ideas
and to obtain instructors’ supplementary information. mooc forums play the role of social learning media for knowledge propagation with increasing number of students and interactions as a course
progresses. every member in the forum can talk about course content with each other, and the intensive interaction between them
supports the knowledge propagation between members of the learning community.
the online discussion forums are usually well structured via the
different threads which are created by students or instructors; they
can contain several posts and comments within the topic. an example of the discussion forum from a famous “machine learning”
course by andre ng on coursera1 is shown in figure 1. the left
figure shows various threads and the right figure illustrates some
replies within the last thread ("having a problem with the collaborative filtering cost"). in general, the replies within a thread are
related to the topic of the thread and they can also refer to some
other threads for supplementary information, like the link in the
second reply. our goal is to point the students towards useful forum threads through effectively mining forum visit patterns.
two aspects set forum recommendation system for moocs apart
from other recommendation scenarios. first, student interests and
preferences drift fast during the span of a course, which is influenced by the dynamics in forums and the content of the course;
second, the pool of items to be recommended and the items them1

https://www.coursera.org/



24

figure 1: an sample discussion forum. left: sample threads. right: replies within the last thread ("having a problem with the collaborative filtering cost").
selves are evolving over time because forum threads can be edited
very frequently by either students or instructors. so the recommendations provided to students need to be adaptive to these drifting
preferences and evolving items. traditional recommendation techniques, such as collaborative filtering and methods based on matrix factorization, only adapt slowly, as they build an increasingly
complex model of users and items. therefore, when a new item is
superseded by a newer version or a new preference pattern appears,
it takes time for recommendations to adapt. to better address the
dynamic nature of recommendation in moocs, we model the recommendation problem as a dynamic and sequential machine learning problem for the task of predicting the next item in a sequence of
items consumed by a user. during the sequential process, the challenge is combining old knowledge with new knowledge such that
both old and new patterns can be identified fast and accurately. we
use algorithms for sequential recommendation based on variableorder markov models. more specifically, we use a structure called
context tree (ct) [21] which was originally proposed for lossless
data compression. we apply the ct method for recommending
discussion forum contents for moocs, where adapting to drifting preferences and dynamic items is crucial. in experiments, it is
compared with various sequential and non-sequential methods. we
show that both old knowledge and new patterns can be captured effectively through context activation using ct, and that this is why
it is particularly strong at adapting to drifting user preferences and
performs extremely well for mooc forum recommendation tasks.
the main contribution of this paper is fourfold:
• we applied the context tree structure to a sequential recommendation tasks where dynamic item sets and drifting user
preferences are of great concern.
• analyze how the dynamic changes in user preferences are
followed in different recommendation techniques.
• extensive experiments are conducted for both sequential and
non-sequential recommendation settings. through the experimental analysis, we validate our hypothesis that the ct
recommender adapts well to drifting preferences.

• partial context matching (pct) technique, built on top of the
standard ct method, is proposed and tested to generalize to
new sequence patterns, and it further boosts the recommendation performance.

2.

related work

typical recommender systems adopt a static view of the recommendation process and treat it as a prediction problem over all historical
preference data. from the perspective of generating adaptive recommendations,we contend that it is more appropriate to view the
recommendation problem as a sequential decision problem. next,
we mainly review some techniques developed for recommender
systems with temporal or sequential considerations.
the most well-known class of recommender system is based on
collaborative filtering (cf) [19]. several attempts have been made
to incorporate temporal components into the collaborative filtering
setting to model users’ drifting preferences over time. a common
way to deal with the temporal nature is to give higher weights to
events that happened recently. [6, 7, 15] introduced algorithms
for item-based cf that compute the time weightings for different
items by adding a tailored decay factor according to the user’s own
purchase behavior. for low dimensional linear factor models, [11]
proposed a model called “timesvd” to predict movie ratings for
netflix by modeling temporal dynamics, including periodic effects,
via matrix factorization. as retraining latent factor models is costly,
one alternative is to learn the parameters and update the decision
function online for each new observation [1, 16]. [10] applied the
online cf method, coupled with an item popularity-aware weighting scheme on missing data, to recommending social web contents
with implicit feedbacks.
markov models are also applied to recommender systems to learn
the transition function over items. [24] treated recommendation as
a univariate time series problem and described a sequential model
with a fixed history. predictions are made by learning a forest of
decision trees, one for each item. when the number of items is big,
this approach does not scale. [17] viewed the problem of generating
recommendations as a sequential decision problem and they con-



25

sidered a finite mixture of markov models with fixed weights. [4]
applied markov models to recommendation tasks using skipping
and weighting techniques for modeling long-distance relationships
within a sequence. a major drawback of these markov models is
that it is not clear how to choose the order of markov chain.
online algorithms for recommendation are also proposed in several literatures. in [18], a q-learning-based travel recommender is
proposed, where trips are ranked using a linear function of several
attributes and the weights are updated according to user feedback.
a multi-armed bandit model called linucb is proposed by [13]
for news recommendation to learn the weights of the linear reward
function, in which news articles are represented as feature vectors;
click-through rates of articles are treated as the payoffs. [20] proposed a similar recommender for music recommendation with rating feedback, called bayes-ucb, that optimizes the nonlinear reward function using bayesian inference. [14] used a markov decision process (mdp) to model the sequential user preferences for
recommending music playlists. however, the exploration phase of
these methods makes them adapt slowly. as user preferences drift
fast in many recommendation setting, it is not effective to explore
all options before generating useful ones.
within the context of recommendation for moocs, [23] proposed
an adaptive feature-based matrix factorization framework for course
forum recommendation, and the adaptation is achieved by utilizing
only recent features. [22] designed a context-aware matrix factorization model to predict student preferences for forum contents, and
the context considered includes only supplementary statistical features about students and forum contents. in this paper, we focus on
a class of recommender systems based on a structure, called context tree [21], which was originally used to estimate variable-order
markov models (vmms) for lossless data compression. then, [2,
12, 5] applied this structure to various discrete sequence prediction tasks. recently it was applied to news recommendation by
[8, 9]. the most important property of online algorithms is the noregret property, meaning that the model learned online is eventually
as good as the best model that could be learned offline. according to [21], the no-regret property is achieved by context trees for
the data compression problem. regret analysis for ct was conducted through simulation by [5] for stochastically generated hidden markov models with small state space. they show that ct
achieves the no-regret property when the environment is stationary.
as we focus on dynamic recommendation environments with timevarying preferences and limited observations, the no-regret property can be hardly achieved while the model adaptation is a bigger
issue for better performance.

3.

context tree recommender

due to the sequential item consumption process, user preferences
can be summarized by the last several items visited. when modeling the process as a fixed-order markov process [17], it is difficult
to select the order. a variable-order markov model (vmm), like a
context tree, alleviates this problem by using a context-dependent
order. the context tree is a space efficient structure to keep track
of the history in a variable-order markov chain so that the data
structure is built incrementally for sequences that actually occur. a
local prediction model, called expert, is assigned to each tree node,
it only gives predictions for users who have consumed the sequence
of items corresponding to the node. in this section, we first introduce how to use the ct structure and the local prediction model for
sequential recommendation. then, we discuss adaptation properties and the model complexity of the ct recommender.

3.1 the context tree data structure

in ct, a sequence s = hn1 , . . . , nl i is an ordered list of items
ni ∈ n consumed by a user. the sequence of items viewed until
time t is st and the set of all possible sequences s.
a context s = {s ∈ s : ξ ≺ s} is the set of all possible sequences
in s ending with the suffix ξ. ξ is the suffix (≺) of s if last elements
of s are equal to ξ. for example, one suffix ξ of the sequence
s = hn2 , n3 , n1 i is given by ξ = hn3 , n1 i.
a context tree t = (v, e) with nodes v and edges e is a partition
tree over all contexts of s. each node i ∈ v in the context tree
corresponds to a context si . if node i is the ancestor of node j then
sj ⊂ si . initially the context tree t only contains a root node
with the most general context. every time a new item is consumed,
the active leaf node is split into a number of subsets, which then
become nodes in the tree. this construction results in a variableorder markov model. figure 2 illustrates a simple ct with some
sequences over an item set hn1 , n2 , n3 i. each node in the ct corresponds to a context. for instance, the node hn1 i represents the
context with all sequences end with item n1 .

figure 2: an example context tree. for the sequence s = hn2 , n3 , n1 i,
nodes in red-dashed are activated.

3.2 context tree for recommendation

for each context si , an expert µi is associated in order to compute
the estimated probability p(nt+1 |st ) of the next item nt+1 under
this context. a user’s browsing history st is matched to the ct and
identifies a path of matching nodes (see figure 2). all the experts
associated with these nodes are called active. the set of active
experts a(st ) = {µi : ξi ≺ st } is the set of experts µi associated
to contexts si = {s : ξi ≺ st } such that ξi are suffix of st . a(st )
is responsible for the prediction for st .

3.2.1

expert model

the standard way for estimating the probability p(nt+1 |st ), as proposed by [5], is to use a dirichlet-multinomial prior for each expert
µi . the probability of viewing an item x depends on the number of
times αxt the item x has been consumed when the expert is active
until time t. the corresponding marginal probability is:
pi (nt+1 = x|st ) = p

αxt + α0
αjt + α0

(1)

j ∈n

where α0 is the initial count of the dirichlet prior



26

3.2.2

combining experts to prediction

when making recommendation for a sequence st , we first identify
the set of contexts and active experts that match the sequence. the
predictions given by all the active experts are combined by mixing
the recommendations given by them:
x
p(nt+1 = x|st ) =
ui (st )pi (nt+1 = x|st )
(2)

in their old contexts. it allows the model to make predictions using more complex contexts as more data is acquired so that old and
new knowledge can be elegantly combined. for new knowledge
or patterns added to an established ct, they can immediately be
identified through context matching. this context organization and
context matching mechanism help new patterns to be recognized to
adapt to changing environments.

i∈a(st )

the mixture coefficient ui (st ) of expert µi is computed in eq. 3
using the weight wi ∈ [0, 1]. weight wi is the probability that
the chosen recommendation stops at node i given that the it can be
generated by the first i experts, and it can be updated in using eq.5.
( q
wi j:sj ⊂si (1 − wj ), if st ∈ si
(3)
ui (st ) =
0,
otherwise
the combined prediction of the first i experts is defined as qi and
it can be computed using the recursion in eq. 4. the recursive
construction that estimates, for each context at a certain depth i,
whether it makes better prediction than the combined prediction
qi−1 from depth i − 1.
qi = wi pi (nt+1 = x|st ) + (1 − wi )qi−1

(4)

the weights are updated by taking into account the success of a
recommendation. when a user consumes a new item x, we update
the weights of the active experts corresponding to the suffix ending
before x according to the probability qi (x) of predicting x sequentially via bayes’ theorem. the weights are updated in closed form
in eq. 5, and a detailed derivation can be found in [5].
wi0 =

3.2.3

wi pi (nt+1 = x|st )
qi (x)

(5)

ct recommender algorithm

the whole recommendation process first goes through all users’ activity sequences over time incrementally to build the ct; the local
experts and weights updated using equations 1 and 5 respectively.
as users browse more contents, more contexts and paths are added
and updated, thus building a deeper, more complete ct. the recommendation for an activity or context in a sequence is generated
using eq. 2 continuously as experts and weights are updated. at
the same time, a pool of candidate items is maintained through a
dynamically evolving context tree. as new items are added, new
branches are created. at the same time, nodes corresponding to old
items are removed as soon as they disappear from the current pool.
the ct recommender is a mixture model. on the one hand, the
prediction p(nt+1 = x|st ) is a mixture of the predictions given
by all the activated experts along the activated path so that it’s a
mixtures of local experts or a mixture of variable order markov
models whose oder are defined by context depths. on the other
hand, one path in a ct can be constructed or updated by multiple
users so that it’s a mixture of users’ preferences.

3.3

adaptation analysis

our hypothesis, which is validated in later experiments, is that the
ct recommender can be applied elegantly to domains where adaptation and timeliness are of concern. two properties of the ct
methods are crucial to the goal. first, the model parameter learning process and recommendations generated are online such that
the model adapts continuously to a dynamic environment. second,
adaptability can be achieved by the ct structure itself as knowledge is organized and activated by context. new items or paths are
recognized in new contexts, whereas old items can still be accessed

3.4 complexity analysis

learning ct uses the recursive update defined in eq. 4 and recommendations are generated by weighting the experts’ predictions
along the activated path given by eq. 2. for trees of depth d, the
time complexity of model learning and prediction for a new observation are both o(d). for input sequence of length t , the updating and recommending complexity are o(m 2 ), where m =
min(d, t ). space complexity in the worst case is exponential to
the depth of the tree. however, as we do not generate branches
unless the sequence occurs in the input, we achieve a much lower
bound determined by the total size of the input. so the space complexity is o(n ), where n is the total number of observations.
compared with the way that markov models are learned, in which
the whole transition matrix needs to be learned simultaneously, the
space efficiency of ct offers us an advantage for model learning.
for tasks that involve very long sequences, we can limit the depth
d of the ct for space and time efficiency.

4.

dataset and problem analysis

4.1 dataset description

in this paper, we work with recommending discussion forum threads
to mooc students. a forum thread can be updated frequently and
it contains multiple posts and comments within the topic. as we
mentioned before that the challenge is adapting to drifting user
preferences and evolving forum threads as a course progresses. for
the experiments elaborated in the following section, we use forum
viewing data from three courses offered by école polytechnique
fédérale de lausanne on coursera. these three courses include
the first offering of “digital signal processing”, the third offering of “functional program design in scala”, and the first offering of “’reactive programming’. they are referred to course 1,
course 2 and course 3. some discussion forum statistics for the
three courses are given in table 1. from the number of forum
participants, forum threads, and thread views, we can see that the
course scale increase from course 1 to course 3. a student on
moocs often accesses course forums many times during the span
of a mooc. each time the threads she views are tracked as one
visit session by the web browser. the total number of visit sessions
and the average session lengths for three courses are presented in
table 1. the length of a session is the number of threads she viewed
within a visit session. the thread viewing sequences corresponding to these regular visit sessions are called separated sequences
in our later experiments and they treat threads in one visit session
as one sequence. models built using separated sequences try to
catch short-term patterns within one visit session and we do not
differentiate the patterns from different students. another setting,
called combined sequences, concatenates all of a student’s visit sessions into one longer sequence so that models built using combined
sequences try to learn long-term patterns across students. the average length of combined sequences is the average session length
times the average number of sessions per student. from course 1
to course 3, average lengths for separated and combined sequences
both increase.



27

# of forum participants
# of forum threads
# of thread views
# of sessions
avg. session length
avg. # of sessions per student

course 1 course 2 course 3
5,399
12,384
13,914
1,116
1,646
2,404
130,093 379,456 777,304
19,892
40,764
30,082
6.5
9
25.8
3.7
3.3
2.2

table 1: course forum statistics for three datasets.
another important issue that we can discover from the statistics is
that thread viewing data available for sequential recommendation is
very sparse. for example in course 1, the average session length is
6.5 and the number of threads is around 1116. then the complete
space to be explored will be 11166.5 , which is much larger than
the size of observations (130,093 thread views). the similar data
sparsity issue is even more severe in the other two datasets.

4.2

forum thread view pattern

next, we study the thread viewing pattern which highlights the significance of adaptation issues for thread recommendation. figure
3 illustrates the distribution of thread views against freshness for
three courses. the freshness of an item is defined as the relative
creation order of all items that have been created so far. for example, when a student views a thread tm which is the m-th thread
created in the currently existing pool of n threads, then freshness
of tm is defined as:
m
(6)
f reshness =
n
we can see from figure 3 that there is a sharp trend that the new
forum threads are viewed much more frequently than the old ones
for all three courses. it is mainly due to the fact that fresh threads
are closely relevant to the current course progress. moreover, fresh
threads can also supersede the contents in some old ones to be
viewed. this tendency to view fresh items leads to drifting user
preferences. such drifting preferences, coupled with the evolving nature of forum contents, requires recommendations adaptive
to drifting or recent preferences.

old. in general, sequential patterns are observed more often within
specific threads as some specific follow-up threads might be related
and useful to the one that you are viewing. so the patterns learned
could be used to guide your forum browsing process. on the contrary, sequential patterns on general threads are relatively random
and imperceptible.
general threads
“using gnu octave”
“any one from india??”
“where is everyone from?
“numerical examples in pdf”
“how to get a certificate”

specific threads
“homework day 1 / question 9”
“quiz for module 4.2”
“quiz -1 question 04”
‘homework 3, question 11”
“week 1: q10 gema problem”

table 2: sample thread titles of general and specific threads.

5. results and evaluation

in this section, we compare the proposed ct method against various baseline methods in both non-sequential and sequential settings. the results show that the ct recommender performs better
than other methods under different setting for all three moocs
considered. through the adaptation analysis, we validate our hypothesis that the superior performance of ct recommender comes
from the adaptation power to drifting preferences and trendy patterns in the domain. in the end, a regularization technique for ct,
called partial context matching (pct), is introduced. it is demonstrated that pct helps better generalize among sequence patterns
and further boost performance.

5.1 baseline methods

5.1.1 non-sequential methods

figure 3: thread viewing activities against freshness

matrix factorization methods proposed by [23, 22] are the state-ofthe-art for moocs course content recommendation. besides the
user-based mf given in [23], we also consider item-based mf that
generates recommendations based on the similarity of the latent
item features learned from standard mf. in our case, each entry in
the user-item matrix of mf contains the number of times a student
views a thread. we also test a version where the matrix had a 1 for
any number of views, but the performance was not as good, so the
development of this version was not taken any further. mf models considered here are updated periodically (week-by-week). to
enable a fair comparison against non-sequential matrix factorization techniques, we implemented versions where the ct model is
updated at fixed time intervals, equal to those of the mf models.
in the “one-shot ct” version, we compute the ct recommendations for each user based on the data available at the time of the
model update, and the user then receives these same recommendations at every future time step until the next update. this mirrors
the conditions of user-based mf. to compare with item-based mf,
the “slow-update ct” version updates the recommendations, but
not the model, at each time point based on the sequential forum
viewing information available at that time.

a further investigation through those views on old threads leads us
to a classification of threads into two categories: general threads
and specific threads. some titles of the general and specific threads
are listed in table 2. we could see the clear difference between
these two classes of threads as the general ones corresponds to
broad topics and specific ones are related to detailed course contents or exercises. we also found that only a very small part of the
old threads are still rather active to be viewed and they are mostly
general ones. different from general threads, specific threads that
subject to a fine timeliness are viewed very few times after they get

sequential methods update model parameters and recommendations continuously as items are consumed. the first two simple
methods are based on the observation and heuristic that fresh threads
are viewed much frequently than old ones. fresh_1 recommends
the last 5 updated threads, and fresh_2 recommends the last 5 created threads. another baseline method, referred as popular, recommends the top 5 threads among the last 100 threads viewed before
the current one. we also consider an online version of mf [10] that

distribution of thread views against freshness

0.3

probability

0.25

course 1
course 2
course 3

0.2
0.15
0.1
0.05
0
0

0.2

0.4

0.6

0.8

1

freshness

5.1.2 sequential methods



28

40%

60%

30%
20%
10%
0%
1

2

3

4

5

6

7

50%
40%

overall perforamance (course 2)

60%

ct
slow-update ct
one-shot ct
item-base mf
user-based mf

succ@5ahead

50%

overall perforamance (course 1)
ct
slow-update ct
one-shot ct
item-base mf
user-based mf

succ@5ahead

succ@5ahead

60%

30%
20%
10%
0%
1

2

3

week

4

5

6

7

50%
40%

overall perforamance (course 3)
ct
slow-update ct
one-shot ct
item-base mf
user-based mf

30%
20%
10%
0%
1

2

3

week

4

5

6

7

week

figure 4: overall performance comparison of ct and non-sequential methods
is currently the state-of-the-art sequential recommendation method,
referred to “online-mf”, in which the corresponding latent factor
of the item i and user u are updated when a new observation rui
arrive. the model optimization is implemented based on elementwise alternating least squares. the number of latent factors is
tuned to be 15, 20, 25 for three datasets, and the regularization parameter is set as 0.01. moreover, the weight of a new observation is
the same as old ones during optimization for achieving the best performance. furthermore, the proposed ct recommender refers to
the full context tree algorithm with a continuously updated model.

5.2

5.2.1

performance and adaptation analysis
evaluation metrics

in our case, all methods recommend top-5 threads each time. two
evaluation metrics are adopted in the following experiments:
• succ@5: the mean average precision (map) of predicting
the immediately next thread view in a sequence.
• succ@5ahead: the map of predicting the future thread
views within a sequence. in this case, a recommendation
is successful even if it is viewed later in a sequence.

5.2.2

comparison of non-sequential methods

figure 4 shows the performance comparison between different versions of methods based on mf and ct on three datasets. “ct”
is the sequential method with a continuously updated model, and
all other methods figure 4 are non-sequential versions. combined
sequences are used for the ct methods here to have a parallel comparison against mf. we found that a small value of the depth limit
of the cts hurts performances, yet a very large depth limit does
not increase performance at the cost of computation and memory.
through experiments, we tune depths empirically and set them as
15, 20, 30 for three datasets.
among non-sequential methods, one-shot ct and user-based mf
perform the worst for all three courses, which means that recommending the same content for the next week without any sequence
consideration is ineffective. slow-update ct performs consistently
the best among non-sequential methods, and it proves that adapting
recommendations through context tree helps boost performance although the model itself is not updated continuously. compared
to slow-update ct, item-based mf performs much worse. they
both update model parameters periodically and the recommendations are adjusted given the current observation. however, using
the contextual information within a sequence and the corresponding prediction experts of slow-update ct are much more powerful
than just using latent item features of item-base mf. moreover, we
can clearly see that the normal ct with continuous update outperforms all other non-sequential methods by a large margin for three
datasets. it means that drifting preferences need to be followed
though continuous and adaptive model update, so sequential methods are better choices. next, we focus on sequential methods, and

we validate our hypothesis that the ct model has superior performances because it better handles drifting user preferences.

5.2.3

comparison of sequential methods

the results presented in table 3 show the performance of the full
ct recommender compared with other sequential baseline methods under different settings and evaluation metrics. each result
tuple contains the performance on the three datasets. we also consider a tail performance metric, referred to personalized evaluation,
where the most popular threads (20, 30, and 40 for three courses)
are excluded from recommendations. the depth limits of cts using separated sequences are set to 8, 10, and 15 for three courses.
we notice that the online-mf method, with continuous model update, performs much worse compared with the ct recommender
for all three datasets. this result shows that matrix factorization,
which is based on interpolation over the user-item matrix, is not
sensitive enough to rapidly drifting preferences with limited observations. the performances of two versions of the fresh recommender are comparable with online-mf, and fresh_1 even outperforms online-mf in many cases, especially for succ@5ahead.
it means that simply recommending fresh items even does a better job than online-mf for this recommendation task with drifting
preferences. we can see that the ct recommender outperforms
all other sequential methods under various settings, except for using non-personalized succ@5ahead for course 2. the popular
recommender is indeed a very strong contender when using nonpersonalized evaluation since there is a bias that students can click a
“top threads” tag from user interface to view popular threads which
are similar to the ones given by popular recommender. from the
educational perspective, the setting using separated sequences and
personalized evaluation is the most interesting as it reflects shotterm visiting patterns within a session over those specific and less
popular forum threads. we could see from the upper right part of
table 3 that the ct recommender outperforms all other methods by
a large margin under this setting.

ct
online-mf
popular
fresh_1
fresh_2
ct
online-mf
popular
fresh_1
fresh_2

non-personalized
personalized
succ@5 succ@5ahead succ@5 succ@5ahead
separated sequences
[25, 23, 21]% [48, 53, 52]% [19, 14, 16]% [41, 37, 42]%
[15, 12, 8]% [33, 29, 23]% [10, 7 ,6 ]% [27, 25, 20]%
[15, 20, 16]% [40, 61, 51]% [9, 8 ,8 ]% [34, 31, 36]%
[12, 14, 10]% [37, 43, 41]% [10, 10, 8]% [33, 31, 37]%
[9, 8, 6]% [31, 31, 29]% [8, 7, 6 ]% [30, 30, 28]%
combined sequences
[21, 20, 20]% [55, 55, 56]% [16, 13, 14]% [46, 39, 46]%
[9, 8, 7]% [34, 27, 23]%
[7,6,6]%
[29, 24, 20]%
[13, 14, 14]% [52, 62, 58]% [9, 8, 7]% [45, 36, 43]%
[10, 12, 9]% [48, 44, 44]% [8, 9, 8]% [44, 34, 42]%
[7, 6, 6]% [43, 34, 32]% [6, 6, 6]% [42, 32, 31]%

table 3: performance comparison of sequential methods

5.2.4

adaptation comparison



29

ct
online-mf

0.8
0.6
0.4
0.2
0
0

0.2

average cdf of recommendation freshness (course 2)

0.4

0.6

0.8

1

1

average cdf of recommendation freshness (course 3)

ct
online-mf

recommended probability

1

recommended probability

recommended probability

average cdf of recommendation freshness (course 1)

0.8
0.6
0.4
0.2
0
0

0.2

freshness

0.4

0.6

0.8

1

1

ct
online-mf

0.8
0.6
0.4
0.2
0
0

0.2

freshness

0.4

0.6

0.8

1

freshness

figure 5: distribution of recommendation freshness of ct and online-mf
0.4

ct
online-mf

0.2
0.1
0
0

0.4

0.3

probability

probability

0.3

p (success|f reshness) for course 2
ct
online-mf

0.2
0.1

0.2

0.4

0.6

0.8

1

0
0

freshness

p (success|f reshness) for course 3
ct
online-mf

0.3

probability

0.4

p (success|f reshness) for course 1

0.2
0.1

0.2

0.4

0.6

0.8

1

0
0

freshness

0.2

0.4

0.6

0.8

1

freshness

figure 6: conditional success rate of ct and online-mf
after seeing the superior performance of the ct recommender, we
move to an insight analysis of the results. to be specific, we compare ct and online-mf in terms of their adaptation capabilities
to new items. figure 5 illustrates the cumulative density function (cdf) of the threads recommended by different methods against
thread freshness. we can see that the cdfs of ct increase sharply
when thread freshness increases, which means that the probability
of recommending fresh items is high compared to online-mf. in
other words, ct recommends more fresh items than online-mf. as
we mentioned before that a large portion of fresh threads are specific ones, instead of general ones, so ct recommends more specific and trendy threads to students while methods based on matrix
factorization recommend more popular and general threads.
other than the quantity of recommending fresh and specific threads,
the quality is crucial as well. figure 6 shows the conditional success rate p (success|f reshness) across different degrees of freshness for three courses. p (success|f reshness) is defined as the
fraction of the items successfully recommended given the item freshness. for instance, if an item with freshness 0.5 is viewed 100 times
throughout a course, then p (success|f reshness = 0.5) = 0.25
means it is among the top 5 recommended items 25 times. as
the freshness increases, the conditional success rate of online-mf
drops speedily while the ct method keeps a solid and stable performance. it is significant that ct outperforms online-mf by a
large margin when freshness is high, in other words, it is particularly strong for recommending fresh items. fresh items are often
not popular in terms of the total number of views at the time point
of recommendation. so identifying fresh items accurately implies
a strong adaptation power to new and evolving forum visiting patterns. the analysis above validates our hypothesis that the ct recommender can adapt well to drifting user preferences. another
conclusion drawn from figure 6 is that the performance of ct is as
good as online-mf for items with low freshness. this is because
that the context organization and context matching mechanism help
old items to be identifiable though old contexts. to conclude, ct is
flexible at combining old knowledge and new knowledge so that it
performances well for items with various freshness, especially for
fresh ones with drifting preferences.

5.3

partial context matching (pct)

at last, we introduce another technique, built on top of the standard ct, to generalize to new sequence patterns and further boost
the recommendation performance. the standard ct recommender
adopts a complete context matching mechanism to identify active
experts for a sequence s. that is, active experts of s come exactly
from the set of suffixes of s. we design a partial context matching (pct) mechanism where active experts of a sequence are not
constrained by exact suffixes, yet they can be those very similar
ones. two reasons bring us to design the pct mechanism for context tree learning. first, pct mechanism is a way of adding regularization. sequential item consumption process does not have to
follow exactly the same order, and slightly different sequences are
also relevant for both model learning and recommendation generation. second, the data sparsity issue we discussed before for sequential recommendation setting can be solved to some extent by
considering similar contexts for learning model experts. the way
pct does aims to activate more experts to train the model, and to
generate recommendations from a mixture of similar contexts.
we will focus on a skip operation that we add on top of the standard
ct recommender. some complex operations, like swapping item
orders, are also tested, but they do not generate better performance.
for a sequence hsp , . . . , s1 i with length p, the skip operation generates p candidate partially matched contexts that skip one sk for
k ∈ [1 . . . p]. all the contexts on the paths from root to partially
matched contexts are activated. for example, the path to context
hn2 , n1 i can be activated from the context hn2 , n3 , n1 i by the skipping n3 . however, for each partially matched context, there may
not exist a fully matched path in the current context tree. in this
case, for each partially matched context, we identify the longest
path that corresponds it with length q. if q/p is larger than some
threshold t, we update experts on this paths and use them to generate recommendations for the current observation. predictions from
multiple paths are combined by averaging the probabilities.

pct-0.5
pct-0.6
pct-0.7
pct-0.8
pct-0.9

success@5
[+0.4, +0.6, +0.2]%
[+0.5, +0.8, +0.3]%
[+0.7, +0.9, +0.5]%
[+0.8, +1.1, +0.6]%
[+1.0, +1.4, +0.7]%

success@5ahead
[+0.8, +0.9, +0.4]%
[+1.1, +1.3, +0.5]%
[+1.6, +1.9, +0.7]%
[+1.9, +2.4, +1.0]%
[+2.0, +2.7, +1.3]%

ratio
[4.9, 4.5, 3.3]
[4.4, 4.1, 2.9]
[3.7, 3.2, 2.5]
[3.2, 2.9, 2.1]
[2.4, 2.2, 1.4]

table 4: performance comparison of pct against ct for three courses



30

table 4 shows the performance of applying pct for both model
update and recommendation with threshold t (pct-t). results are
compared with the full ct recommender with separated sequences
and non-personalized evaluation. for cases where the threshold is
smaller than 0.5, we sometimes obtain negative results since partially matched contexts are too short to be relevant. the “ratio”
column is the ratio of the number of updated paths in pct compared with standard ct. we can see that pct updates more paths
and it offers us consistent performance boosts at the cost of computation.

6. conclusion and future work

in this paper, we formulate the mooc forum recommendation
problem as a sequential decision problem. through experimental
analysis, both performance boost and adaptation to drifting preferences are achieved using a new method called context tree. furthermore, a partial context matching mechanism is studied to allow a
mixture of different but similar paths. as a future work, exploratory
algorithms are interesting to be tried. as exploring all options for
all contexts are not feasible, we consider to explore only those top
options from similar contexts. deploying the ct recommender in
some moocs for online evaluation would be precious to obtain
more realistic evaluation.

7. references

[1] j. abernethy, k. canini, j. langford, and a. simma. online
collaborative filtering. university of california at berkeley,
tech. rep, 2007.
[2] r. begleiter, r. el-yaniv, and g. yona. on prediction using
variable order markov models. journal of artificial
intelligence research, pages 385–421, 2004.
[3] r. m. bell, y. koren, and c. volinsky. the bellkor 2008
solution to the netflix prize. statistics research department
at at&t research, 2008.
[4] g. bonnin, a. brun, and a. boyer. a low-order markov
model integrating long-distance histories for collaborative
recommender systems. in international conference on
intelligent user interfaces, pages 57–66. acm, 2009.
[5] c. dimitrakakis. bayesian variable order markov models. in
international conference on artificial intelligence and
statistics, pages 161–168, 2010.
[6] y. ding and x. li. time weight collaborative filtering. in
acm international conference on information and
knowledge management, pages 485–492. acm, 2005.
[7] y. ding, x. li, and m. e. orlowska. recency-based
collaborative filtering. in australasian database conference,
pages 99–107. australian computer society, inc., 2006.
[8] f. garcin, c. dimitrakakis, and b. faltings. personalized
news recommendation with context trees. in acm
conference on recommender systems, pages 105–112.
acm, 2013.
[9] f. garcin, b. faltings, o. donatsch, a. alazzawi, c. bruttin,
and a. huber. offline and online evaluation of news
recommender systems at swissinfo.ch. in acm conference
on recommender systems, pages 169–176. acm, 2014.
[10] x. he, h. zhang, m.-y. kan, and t.-s. chua. fast matrix
factorization for online recommendation with implicit
feedback. in international acm conference on research and
development in information retrieval, volume 16, 2016.
[11] y. koren. collaborative filtering with temporal dynamics.
communications of the acm, 53(4):89–97, 2010.

[12] s. s. kozat, a. c. singer, and g. c. zeitler. universal
piecewise linear prediction via context trees. ieee
transactions on signal processing, 55(7):3730–3745, 2007.
[13] l. li, w. chu, j. langford, and r. e. schapire. a
contextual-bandit approach to personalized news article
recommendation. in international conference on world
wide web, pages 661–670. acm, 2010.
[14] e. liebman, m. saar-tsechansky, and p. stone. dj-mc: a
reinforcement-learning agent for music playlist
recommendation. in international conference on
autonomous agents and multiagent systems, pages 591–599.
ifaamas, 2015.
[15] n. n. liu, m. zhao, e. xiang, and q. yang. online
evolutionary collaborative filtering. in acm conference on
recommender systems, pages 95–102. acm, 2010.
[16] j. mairal, f. bach, j. ponce, and g. sapiro. online learning
for matrix factorization and sparse coding. journal of
machine learning research, 11(jan):19–60, 2010.
[17] g. shani, r. i. brafman, and d. heckerman. an mdp-based
recommender system. in conference on uncertainty in
artificial intelligence, pages 453–460. morgan kaufmann
publishers inc., 2002.
[18] a. srivihok and p. sukonmanee. e-commerce intelligent
agent: personalization travel support agent using
q-learning. in international conference on electronic
commerce, pages 287–292. acm, 2005.
[19] x. su and t. m. khoshgoftaar. a survey of collaborative
filtering techniques. advances in artificial intelligence,
2009:4, 2009.
[20] x. wang, y. wang, d. hsu, and y. wang. exploration in
interactive personalized music recommendation: a
reinforcement learning approach. acm transactions on
multimedia computing, communications, and applications,
11(1):7, 2014.
[21] f. m. willems, y. m. shtarkov, and t. j. tjalkens. the
context-tree weighting method: basic properties. ieee
transactions on information theory, 41(3):653–664, 1995.
[22] d. yang, d. adamson, and c. p. rosé. question
recommendation with constraints for massive open online
courses. in acm conference on recommender systems,
pages 49–56. acm, 2014.
[23] d. yang, m. piergallini, i. howley, and c. rose. forum
thread recommendation for massive open online courses. in
educational data mining, 2014.
[24] a. zimdars, d. m. chickering, and c. meek. using temporal
data for making recommendations. in conference on
uncertainty in artificial intelligence, pages 580–588.
morgan kaufmann publishers inc., 2001.



31

analysis of problem-solving behavior in open-ended
scientific-discovery game challenges
aaron bauer

awb@cs.washington.edu

jeff flatten

jflat06@cs.washington.edu

zoran popović

zoran@cs.washington.edu

center for game science, computer science and engineering
university of washington
seattle, wa 98195, usa

abstract

problem-solving skills in creative, open-ended domains are both
important and little understood. these domains are generally illstructured, have extremely large exploration spaces, and require
high levels of specialized skill in order to produce quality solutions.
we investigate problem-solving behavior in one such domain, the
scientific-discovery game foldit. our goal is to discover differentiating patterns and understand what distinguishes high and low levels
of problem-solving skill. to address the challenges posed by the
scale, complexity, and ill-structuredness of foldit solver behavior
data, we devise an iterative visualization-based methodology and use
this methodology to design a concise, meaning-rich visualization of
the problem-solving process in foldit. we use this visualization to
identify key patterns in problem-solving approaches, and report how
these patterns distinguish high-performing solvers in this domain.

keywords

problem solving; scientific-discovery games; visualization

1.

introduction

as efforts in scalable online education expand, interest continues
to increase in moving beyond small, highly constrained tasks, such
as multiple choice or short answer questions, and incorporating
creative, open-ended activities [7, 14]. existing research supports
this move, showing that problem-based learning can enhance students’ problem-solving and metacognitive skills [11]. scaling such
activities poses significant challenges, however, in terms of both assessment and feedback. it will be vital to devise scalable techniques
not only to assess students’ final products, but also to understand
their progress through complex and heterogeneous problem-solving
spaces. these techniques will apply to a broad range of education
settings, from purely online programs like udacity’s nanodegrees
to more traditional settings where new standards like the common
core emphasize strategic problem solving.
a growing body of work has found that educational and serious
games are fertile ground for assessing students’ capabilities and
problem-solving skills [6, 10]. our work continues this general
line of inquiry by examining creative, problem-solving behavior
among players in the scientific-discovery game foldit. by modeling
the functions of proteins, the workhorses of living cells, foldit
challenges players, hereafter referred to as solvers, to resolve the
shape of proteins as a 3d puzzle. these puzzles are completely
open and often under-specified, making it a highly suitable setting
in which to gain insight into student progress through complex
solution spaces. in the foldit scientific-discovery community, the
focus is on developing people from novices to experts that are
eventually capable of solving protein structure problems that are

currently unsolved by the scientific community. in fact, solutions
produced in foldit have led to three results published in nature [3,
5, 16]. foldit is an attractive learning space domain because its
solvers are capable of contributing to state-of-the-art biochemistry
results, and the vast majority of best performing solvers had no
exposure to biochemistry prior to joining foldit community. hence,
solver behavior in foldit represents development of highly effective
problem-solving in an open-ended domain over long time horizons.
in this work, we identify six strategic patterns employed by foldit
solvers and show how these patterns differentiate between successful
and less successful solvers. these patterns cover instances where
solvers investigate multiple hypotheses, explore more greedily or
more inquisitively, try to escape local optima, and make structured
use of the manual or automated tools available in foldit.
the aspects of the foldit environment that make it an attractive
setting in which to study problem solving also present significant
challenges. problems in foldit share many of the properties jonassen
attributes to design problems, which they describe as “among the
most complex and ill-structured kinds of problems that are encountered in practice” [13]. these properties include a vague goal with
few constraints (in foldit, the goal is often entirely open-ended:
find a good configuration of the protein), answers that are neither
right or wrong, only better or worse, and limited feedback (in foldit,
real-time feedback and solution evaluation are limited to a single
numerical score corresponding to the protein’s current energy state,
and solvers frequently must progress through many low-scoring
states to reach a good configuration; more nuanced feedback from
biochemists is sometimes available, but on a timescale of weeks).
the ill-structured nature of problems posed in foldit necessarily
deprives us of the structures, such as clear goal states and straightforward relationships between intermediate states and goal states,
that typically form the basis of existing detailed and quantitative
analyses of problem-solving behavior.
the size and complexity of foldit’s problem space presents another
major challenge. even though the logs of solver interactions consist
only of regular snapshots of a solver’s current solution (along with
attendant metadata), the record of a single solver’s performance on
a given problem frequently consists of thousands of such snapshots
(which in turn are just a sparse sampling of the actual solving process). furthermore, the nature of the solution state, the configuration
of hundreds of components in continuous three-dimensional space,
renders collapsing the state space by directly comparing solution
states impractical. compounding the size of the problem space is
the complexity of the actions available to foldit solvers. in addition
to manual manipulation of the protein configuration, solvers can
invoke various low-level automated optimization routines (some



32

of which run until the solver terminates them) and place different
kinds of constraints on the protein configuration (rubber bands in
foldit parlance) that restrict its modification in a variety of ways.
solvers can also deploy many of these tools programmatically via
lua scripts called recipes. taken together these challenges of illstructuredness, size, and complexity threaten to make analysis of
high-level problem-solving behavior in foldit intractable.
to overcome these obstacles, we devise a visualization-based methodology capable of producing tractable representations of foldit solvers’
problem-solving behavior while maintaining the key encodings necessary for analysis of high-level strategic behavior. a process of
iterative summarization forms the core of this methodology, and
ensures that the transformations applied to the raw data do not
elide structures potentially relevant to understanding solvers’ unique
strategic behavior. using this methodology, we examine solver activity logs from 11 foldit puzzles, representing 970 distinct solvers and
nearly 3 million solution snapshots. leveraging metadata present
in the solution snapshots, we represent solving behavior as a tree,
and apply our methodology to visualize a summarized tree showing
where they branched off to investigate multiple hypotheses, how
they employed some of the automated tools available to them, and
other salient problem-solving behavior. we use these depictions to
determine key distinguishing features of this exploration process.
we subsequently use these features to better understand the patterns
of expert-level problem solving.
our work focuses on the following research questions: (1) how
can we visually represent an open-ended exploration towards a
high-quality solution in a large, ill-structured problem space? (2)
what are the key patterns of problem-solving behavior exhibited
by individuals?, and (3) what are the key differences along these
patterns between high-performing and lower-performing solvers in
an open-ended domain like foldit? in addressing these questions we
find that high-performing solvers explore the solution space more
broadly. in particular, they pursue more hypotheses and actively
avoid getting stuck in local minima. we also found that both highand lower-performing solvers have similar proportion of manual and
automated tool actions, indicating that better performance on openended challenges stems from the quality of the action intermixing
rather than aggregate quantity.

2.

related work

while automated grading has mostly been explored for well-specified
tasks where the correct answer has a straightforward and concise
description, some previous work has developed techniques for more
complex activities. some achieve scalability through a crowdsourcing framework such as udacity’s system for hiring external
experts as project reviewers [14]. other work has demonstrated
automated approaches that leverage machine learning to enable scalable grading of more complex assignments. for example, geigle et
al. describe an application of online active learning to minimize the
training set a human grader must produce [7] when automatically
grading an assignment where students must analyze medical cases.
our work does not focus on grading problem-solving behavior, but
instead approaches the issue of scalability at a more fundamental
level: understanding fine-grained problem-solving strategies and
how they contribute to success in an open-ended domain.
a robust body of prior work has addressed the challenge of both
visualizing and gleaning insight from player activity in educational
and serious games. andersen et al. developed playtracer, a general method for visualizing players’ progress through a game’s

state space when a spatial relationship between the player and the
virtual environment is not available [1]. wallner and kriglstein provide a thorough review of visualization-based analysis of gameplay
data [21]. prior work has analyzed gameplay data without visualization as well. falakmasir et al. propose a data analysis pipeline
for modeling player behavior in educational games. this system
can produce a simple, interpretable model of in-game actions that
can predict learning outcomes [6]. our work differs in its aims from
this prior work. we do not seek to develop a general visualization
technique, but instead to design and leverage a domain-specific
visualization to analyze problem-solving behavior. we are also
not predicting player behavior, nor modeling players in terms of
low-level actions, but rather identifying higher-level strategy use.
the work most similar to ours is that which focuses on problemsolving behavior, including both the long-running efforts in educational psychology to develop general theories and more recent
work data-driven on understanding the problem-solving process.
our formulation of solving behavior in foldit as a search through
a problem space follows from classic information-processing theories of problem solving (e.g., [9, 19]). gick reviews research on
both problem-solving strategies and the differences in strategy use
between experts and novices [8]. our work complements the existing literature by focusing on understanding problem solving in
the little-studied domain of scientific-discovery games, and on the
ill-structured problems present in foldit. our findings on the differences in strategy use between high- and lower-performing solvers in
foldit are consistent with the consensus in the literature that expert’s
knowledge allows them to effectively use strategies that are poorly
or infrequently used by less-skilled solvers. we also contribute a
granular understanding of the specific strategies and differences at
work in the foldit domain.
significant recent work has investigated problem-solving behavior
in educational games and intelligent tutoring systems using a variety
of techniques. tóth et al. used clustering to characterize problemsolving behavior on tasks related to understanding a system of linear
structural equations. the clusters distinguished between students
that used a vary-one-thing-at-a-time strategy (both more and less
efficiently) and those that used other strategies [20]. through a
combination of automated detectors, path analysis, and classroom
studies, rowe et al. investigated the relationship between a set
of six strategic moves in a newtonian physics simulation game
and performance on pre- and post-assessments. they found that
the use of some moves mediated the relationship between prior
achievement and post scores [18]. eagle et al. discuss several applications of using interaction networks to visualize and categorize
problem-solving behavior in education games and intelligent tutoring systems. these networks offer insight for hint generation
and a flexible method for visualizing student work in rule-using
problem solving environments [4] . using decision trees to build
separate models for optimal and non-optimal student performance,
malkiewich et al. gained insight into how learning environments
can encourage elegant problem solving [17]. our primary contribution is to extend analysis of problem-solving behavior to a more
complex and open-ended domain that those studied in similar previous work. the size and complexity of foldit’s problem space,
the volume of data necessary to capture exploration in this space,
and the ill-structured nature of the foldit problems all pose unique
challenges. we devise a visualization-based methodology focused
on iterative summarization, and successfully apply it to identify key
problem-solving patterns exhibited by foldit solvers.



33

3.

foldit

foldit is a scientific-discovery game that crowdsources protein folding. it presents solvers with a 3d representation of a protein and
tasks them with manipulating it into the lowest energy configuration. each protein posed to the solvers is called a puzzle. solvers’
solutions to each puzzle are scored according to their energy configuration, and solvers compete to produce the highest scoring results.

figure 1: the foldit interface. foldit solvers use a variety of
tools to interactively reshape proteins. in this figure, a solver
uses rubber bands to pull together two sheets, long flat regions
of the protein.
solvers have many tools at their disposal when solving foldit puzzles. they can manipulate and constrain the structure in various
ways, employ low-level automated optimization (e.g., a wiggle tool
makes small, rapid, local adjustments to try and improve the score),
and trigger solver-created automated scripts called recipes that can
programmatically use the other tools. there is, however, a subset of
the basic actions that cannot be used by recipes. we will call these
manual-only actions. previous work analyzing solver behavior in
foldit has focused primarily on recipe use and dissemination [2] and
recipe authoring [15].
foldit has several different types of puzzles for solvers to solve. in
this work, we focus on the most common type of puzzle, prediction
puzzles. these are puzzles in which biochemists know the amino
acids that compose the protein in question, but do not know how
the particular protein folds up in 3d space. this is in contrast to
design puzzles in which solvers insert and delete which amino acids
compose the protein to satisfy a variety of scientific goals, including
designing new materials and targeting problematic molecules in
diseases. we focus on prediction puzzles in this work to simplify
our analysis by having a consistent objective (i.e., maximize score)
across the problem-solving behavior we analyze.

4.

methodology

prior work has demonstrated the power of visualization to support
understanding of problem-solving behavior (e.g., [12]). hence, we
devise a methodology capable of producing concise, meaning-rich
visualizations of the problem-solving process in foldit, and then
leverage these visualizations to identify key patterns of solver behavior. we are specifically interested in how solvers navigate from
a puzzle’s start state to a high-quality solution, what states they
pass through in between, and what other avenues they explored.

since solving a foldit puzzle can be represented as a directed search
through a problem space, the clear encoding of parent-child relationships between nodes offered by a tree make it well-suited for
visualizing these aspects of the solving process.
the scale of the foldit data necessitates significant transformation
of the raw data in order to render concise visualizations. without
any transformation, meaningful patterns are overwhelmed by sparse,
repetitive data and would be far more challenging to identify. while
there are many existing techniques for large-scale tree visualization,
we find clear benefits to developing a visualization tailored to the
foldit domain. specifically, preserving the semantics of our visual
encoding is crucial for allowing us to connect patterns in the visualization to concrete strategic behavior in foldit. to accomplish this,
the process by which concise visualization are constructed must
be carefully designed to maintain these links. hence, we devise a
design methodology focused on iterative summarization.
this process begins by visualizing the raw data. this is followed
by iteratively building and refining a set of transformations to summarize the raw data while preserving meaning. the design of these
transformations should be guided by frequently occurring structures.
that is, those structures that the transformations can condense without eliding structures corresponding to unique strategic behavior.
in parallel to this iterative design, a set of visual encodings are developed to represent the solving process as richly as possible. key
to this entire process is frequent consultation with domain experts,
in our case experts on foldit and its community. by applying this
iterative methodology for several cycles, we designed a domainspecific visualization that we use to identify patterns of strategic
behavior among foldit solvers. we follow up on these patterns with
computational investigation, and quantify their application by highand lower-performing solvers.

4.1

data

for our analysis, we selected 11 prediction puzzles spanning the
range of time for which the necessary data is available. though
foldit has been in continuous use since 2010, the data necessary to
track a solver’s progress through the problem space has only been
collected since mid-2015. our chosen dataset represents 970 unique
solvers and nearly 3 million solution snapshots. these 11 puzzles are
just a small subset of the available foldit data. we chose a subset of
similar puzzles (i.e., a subtype of relatively less complex prediction
puzzles) in order to make common solving-behavior patterns easier
to identify. the size of the subset was also guided by practical
constraints, as each puzzle constitutes a large amount of data (20-60
gb for the data from all players on a single puzzle).
the data logged by foldit primarily consists of snapshots of solver
solutions as they play, stored as text files using the protein data
bank (pdb) format. these snapshots include the current protein
pose, a timestamp, the solution’s score, the number of times the
solver has invoked each action and recipe, and a record of the intermediate states that led up to the solution at the time of the snapshot.
this record, or solution history, is a list of unique identifiers each
corresponding to a previous solution state. this list is extended
every time the solver undoes an action or reloads a previous solution.
hence, by comparing the histories of two snapshots from the same
solver, we can answer questions about their relationship (e.g., does
one snapshot represent the predecessor of another; where did two
related snapshots diverge). the key relationship for the purposes of
this analysis is the direct parent-child relationship, which we use to
generate trees that represent a solver’s solving process.



34

4.2

visualizing solution trees

we applied our methodology to our chosen subset of foldit data to
design a visualization of an individual’s problem-solving process
as a solution tree. several key principles guided this design. first,
since our goal is to discover key patterns, the visualization needs
to highlight distinctly different strategies and approaches. these
differences cannot be buried amidst enormous structures, nor destroyed by graph transformations. second, the visualization must
depict the closeness of each step to the ultimate solution in both time
and quality to give a sense of the solver’s progression. third, the
solver’s use of automation in the form of recipes should be apparent
since the use of automation is an important part of foldit.
the fundamental organization of the visualization is that each node
corresponds to a solution state encountered while solving. using the
solution history present in the logged snapshots of solver solutions,
we establish parent-child relationships between solutions. if solution
β is a child of solution α, it indicates that β was generated when
the solver performed actions on α. one crucial limitation, however,
is that a snapshot of the solver’s current solution is captured far less
often (only once every two minutes) than the solver takes actions.
this means that our data is sparsely distributed along a solution’s
history going back to the puzzle’s starting state. hence, when naively
constructing the tree from the logged solution histories, it ends up
dominated by vast quantities of nodes with no associated data.
we address this issue by performing summarization on the solution
trees, condensing them into concise representations amenable to
analysis for important features. this summarization takes place
in two stages. the first stage trims out nodes that (1) do not have
corresponding data and (2) have zero children. this eliminates
large numbers of leaf nodes that we are unable to reason about
given that we lack the corresponding data. this stage also combines
sequences of nodes each with only one child into a single node. for
the median tree, this stage reduced the number of nodes by an order
of magnitude from over 12,000 nodes to about 1,600.
the second stage consists of four phases, each informed by our
observations of common patterns in trees produced by the first stage
that would benefit from summarization. the first phase, called
prune, focuses on simplifying uninteresting branches. we observed
many of the branches preserved by the first stage were small, with
at most three children, and only continued the tree from one of
those children. prune removes the leaf children of these branches
from the tree. collapse, the second phase, transforms each of the
sequences of single-child nodes left behind after prune into single
nodes. the third phase, condense, targets another common pattern
where a sequence of branches feed into each other, with a child of
each branch the parent of the next branch. these sequences are
summarized into a single node labeled cascade along with the
depth (number of branches) and width (average branching factor)
of the summarized branches. see figure 2 for an example of the
features summarized by these three phases. the final phase, clean,
targets the ubiquitous empty nodes (i.e., nodes for which we lack
associated data) shown in black in figure 2. we eliminate them by
merging them with their parent node, doing so repeatedly until they
all have been merged into nodes that contain data. in addition to
making the trees more concise, this step allows us to reason more
fully over the trees since all nodes are guaranteed to contain data.
this second stage of summarization further reduced the number of
nodes in the median tree by another order of magnitude to about
300 nodes. summarization similarly reduces the space required to
store the data by two orders of magnitude.

figure 2: a solution tree after only the first stage of summarization. the non-black node color represents the score of the
solution at that node (red is worse). the black nodes are empty
in that we do not have solution data corresponding to that node.
this figure also shows examples of the features targeted by the
second summarization stage: prune and collapse eliminate long
chains like the one on the right, and condense combines sequences of branches like those going down to left in single cascade nodes.
child-parent relationships are not the only part of the data we visually encoded in the solution trees. nodes are colored on a continuous
gradient from red to blue according to the score of the solution represented by that node (red is low-scoring, blue is high-scoring). the
best-scoring node is highlighted as a yellow star. edges are colored
on a continuous gradient from light to dark green according to the
time the corresponding transition took place, and the children of
each node are arranged left to right in chronological order. finally,
use of automation via recipes is an important aspect of problemsolving in foldit. since the logged solution snapshots contain a
record of which recipes have been used at that point, we can use this
to annotate nodes where a recipe was triggered. the annotations
consist of the id of that recipe (a 4 to 6 digit number) and the number
of times it was started.
one major weakness in the data available to us is the lack of a consistent way to determine when the execution of a recipe ended (some
recipes save and restore, possibly being responsible for multiple
nodes in the graph beyond where they were triggered). we partially
address this by further annotating a node with the label manual
whenever the solver took a manual-only action at that node. this
indicates that no previously triggered recipe continued past that node
because no recipe could have performed the manual-only action.
since nodes in the summarized trees can represent many individual
steps, it is possible for them to have several of these recipe and
manual action annotations.

5.

results

using visualized solution trees for a large set of solvers across our
sample of 11 puzzles, we identify a set of six prominent patterns in
solvers’ problem-solving behavior. these patterns do not encompass
all solving behavior in foldit, but instead capture key instances of
strategic behavior in three categories: exploration, optimization, and
human-computer collaboration. future work is needed to generate
a comprehensive survey of the strategic patterns in these and other
categories. in this analysis, our focus is on identifying a small,
diverse set of commonly occurring patterns to both provide initial



35

insight into problem-solving behavior, and to demonstrate the potential of our approach. in addition to identification, we also perform
a quantitative comparison of how these patterns are employed by
high-performing and lower-performing solvers to gain an understanding of how these patterns contribute to success in an open-end
environment like foldit.

5.1

has a high-scoring node with a low-scoring child, and then chooses
to explore from the low-scoring child. the solver was willing to
ignore the short-term drop in score to try and reach a more beneficial
state in the long-term. figure 5 gives an example of this pattern.

problem-solving patterns

exploration. foldit solvers are confronted with a highly discon-

tinuous solution space with many local optima, creating a trade-off
between narrowly focusing their efforts or taking the time to explore
a broader range of possibilities. in our first two patterns, we examine the broader exploration side of this trade-off at two different
scales. taking the macro-scale first, we identify a pattern where
solvers make significant progress on distinct branches of the tree
(see figure 3 for an example). we interpret this pattern as the solver
investigating multiple hypotheses about the puzzle solution, using
multiple instances of the game client or foldit’s save and restore features to deeply explore them all. we call this the multiple hypotheses
pattern.

figure 5: an example of the optima escape pattern. the solver
transitions from a relatively high-scoring (i.e., blue) state in the
upper left to a low-scoring (i.e., red) state. what makes this
an example of the pattern is that exploration from the lowscoring state. in this case, the perseverance paid off as the
solver reaches even higher-scoring states in the lower right.
in the other direction, we identify the greedy pattern in which solvers
exclusively explore from the best-scoring of the available options.
obviously, some amount of greedy exploration is necessary in order
to refine solutions, but in its extreme form deserves recognition
as a pattern with significant potential impact on problem-solving
success. naturally, these two patterns do not cover all the ways
solvers explore the problem space, but they do characterize specific
strategic behavior of interest in this analysis.

figure 3: an example of the multiple hypotheses pattern. the
two hypotheses branch out one of the nodes at the top and continue to the left (a) and right (b).
at the micro-scale, solvers very frequently generate a large number
of possible next steps (i.e., a branch with a large number of children),
but most often proceed to explore only one of them further. this is
natural given the iterative refinement needed to successfully participate in foldit. hence, solvers that exhibit a pattern of much more
frequently exploring multiple local possibilities demonstrate an unusual effort to explore more broadly. we call this the inquisitive
pattern. figure 4 shows an example of this behavior.

figure 6: an example of the repeated recipe pattern. at three
points in this solution tree snippet, the solver applies recipe
49233 to every child of a node.

human-computer collaboration. human-computer collabo-

figure 4: an example of the inquisitive pattern. note how frequently multiple children of the same node are explored when
compared to the tree in figure 3.

optimization. navigating the extremely heterogeneous solution

space is the primary challenge in foldit, so we look closely at how
solvers attempt to optimize their solutions, digging deeper into
solvers’ approach to exploration than the previous two patterns.
we identify two related patterns describing solvers’ fine-grained
approach to optimization. the solution spaces of foldit puzzles
contain numerous local optima that solvers must escape, and we
identify an optima escape pattern highly suggestive of a deliberate
attempt to escape a local optima. this pattern occurs when a solver

ration is a vital part of foldit, and managing the trade-off between
automation and manual intervention is a key feature of solving
foldit puzzles. we identify two patterns that each focus on one
side of this trade-off. the first, the manual pattern, corresponds to
extended sections of exclusively manual exploration. since recipe
use is very common, extended manual exploration represents a significant investment in the manual intervention side of the trade-off.
limitations with foldit logging data prevent us from capturing all
the manual exploration (i.e., it is not always possible to determine
whether an action was performed by a solver manually or triggered
as part of an automated recipe), but what can be captured is still an
important dimension of variance among problem-solving behavior.
our final pattern concerns recipe use. some solvers apply a recipe
to every child of a node periodically throughout their solution tree,
using it as a clean-up or refinement step before continuing on (see
figure 6). we call this the repeated recipe pattern. recipe use is
very diverse and frequently doesn’t display any specific structure,
making this pattern interesting for its regimented way of managing
some of the automation while solving.



36

figure 7: the number of hypotheses pursued in each solution
tree for high- and lower-performing solvers. high-performing
solvers frequently pursue two or more hypotheses, whereas
lower-performing solvers most often pursue just one. red circles show the distribution of individual solvers.

5.2

problem-solving patterns and
solver performance

to understand how the patterns we identify relate to skillful problemsolving in an open-ended domain like foldit, we compare their use
among high-performing solvers to that among lower-performing
solvers. specifically, we analyze the occurrence of these patterns in
the 15 best-scoring solutions from each puzzle and compare that to
the occurrence in solutions from each puzzle ranked from 36th to
50th. though it varies somewhat between puzzles, in general the
solutions ranked 36th to 50th represent a middle ground in terms
of quality. they fall outside the puzzle’s state-of-the-art solutions,
but remain well above the least successful efforts. throughout these
comparisons we use non-parametric mann-whitney u tests with
α = 0.008 confidence (bonferroni correction for six comparisons,
α = 0.05/6), as our data is not normally distributed. for each test,
we report the test statistic u, the two-tailed significance p, and the
rank-biserial correlation measure of effect size r. in addition, since
some of the metrics we compute may not apply to all solution trees
(e.g., the tree contains no branches where the inquisitive pattern
can be evaluated), we report the number of solvers involved in the
comparison n for each test (the full sample is n = 330).
we find high-performing solvers explore more broadly than lowerperforming solvers. for the multiple hypotheses pattern, highperforming solvers pursued significantly more hypotheses than
lower-performing solvers (u = 10569, p = 0.000014, r = 0.217,
n = 330) (see figure 7). for the inquisitive pattern, we compute
the proportion of each solver’s exploration that matches the pattern
(i.e., of all the branches in a solver’s solution tree, in what fraction of them did the solver explore more than one child) and find
high-performing solvers explore inquisitively more often than lowerperforming solvers (u = 9343, p = 0.000295, r = 0.231, n = 313)

figure 8: the proportion of all the branches in a solver’s solution tree in which the solver explored more than one child
for high- and lower-performing solvers. red circles show the
distribution of individual solvers.
(see figure 8).
we also find high-performing solvers work harder to avoid local
optima. for the optima escape pattern, we compute the number of times this behavior occurs in each solution and find that
high-performing solvers engage in this behavior more than lowerperforming solvers (u = 11183.5, p = 0.00185, r = 0.173, n = 330)
(see figure 9). for the greedy pattern, we compute the proportion of each solver’s exploration that matches the pattern (i.e., of
all the branches in a solver’s solution tree, in what fraction of
them did the solver only explore the best-scoring child). while
high-performing solvers engaged in greedy optimization less often
than lower-performing solvers, the difference was not significant
(u = 9079, p = 0.0158, r = −0.163, n = 295) (see figure 10).
finally, we find no significant difference between high- and lowerperforming solvers in the frequency they manually explore and
employ recipes. for the manual pattern, we compute the number of
manual exploration sections in each solution and find no significant
difference between high- and lower-performing solvers (u = 13334,
p = 0.789, r = 0.014, n = 330). for the repeated recipe pattern,
we computed the median frequency of recipe use along all paths
in the solution (i.e., for each path from the root to a leaf, in what
fraction of the nodes did the solver trigger at least one recipe) and
though lower-performing solvers used recipes more frequently, the
difference between high- and lower-performing solvers was not
significant (u = 11342, p = 0.0140, r = −0.157, n = 329).

6.

discussion

the results from our analysis of our solution tree visualizations illuminate some key problem-solving patterns exhibited by individual
foldit solvers. namely, how broadly an individual explores, both
on a macro- and micro-scale, how actively an individual avoids



37

figure 9: the number of times in each solution a solver engages in optima escape behavior for high- and lower-performing
solvers. red circles show the distribution of individual solvers.

local optima by engaging in less greedy optimization and actively
pursuing locally suboptimal lines of inquiry, and how an individual
manages the interplay between automation and manual intervention.
comparing high- and lower-performing solvers in their application of these patterns suggests that skillful problem-solving in an
open-end domain like foldit involves broader exploration and more
conscious avoidance of local minima. this finding that a key feature
of high-skill solving behaviors is not being enamored by the current
best solution and possessing strategies for avoiding myopic thinking
had implications for the strategies that should be taught to develop
successful problem solvers. further work is required on other large
open-ended domains to confirm this trend.
the finding that solvers of different skill use greedy exploration,
manual exploration, and automation in similar amounts suggests
skillful deployment of non-greedy exploration, automation, and
manual intervention takes place at a more fine-grained level than
overall quantity. though this work focuses on the presence or
absence of specific solving behavior, the timing and sequencing of
strategic moves are likely to be critical to success. further work is
needed to investigate what differentiates effective and ineffective
use of specific solving strategies.
the foldit dataset itself presented significant challenges for our
analysis, and we addressed these through an iterative visualizationbased methodology. this process served as a design method for
generating a visual grammar to describe a complex problem-solving
process. we do not study the generalization of this approach to
other datasets and domains in this work, but the prerequisites for
its application to other open-ended problem-solving domains can
be concisely enumerated: (1) the logs of solver activity establish
clear temporal relationships between solution states such that those
states can be visualized as a progression through the solution space,

figure 10: the proportion of all the branches in a solver’s solution tree in which the solver explored only the best-scoring
child for high- and lower-performing solvers. the fact that the
median for both categories of solver is above 0.5 indicates that
this pattern in an important part of refining solutions in foldit.
red circles show the distribution of individual solvers.
(2) the solution state or associated metadata is amenable to visual
encoding, so that the visualized progressions can represent finegrained details of the solving process, and (3) deep problem-solving
domain expertise is available to provide the necessary context for
interpreting and summarizing the visualized structures.
our chosen subset of foldit data represents only a small fraction
of the total available data. in particular, we limited our analysis
to a sample of similar prediction puzzles, and compared specific
ranges of high- and lower-performing solvers. though these choices
are well-motivated, it is an important question for future work as
to whether our results hold across different datasets and groups of
comparison. more broadly, foldit supports numerous variations
on the prediction and design puzzle archetypes, which offers an
exciting opportunity to study problem solving across a number of
related contexts with varying goals, constraints, inputs, and tools.

7.

conclusion

gaining a better understanding of key patterns in problem-solving
behavior in complex, open-ended environments is important for deploying this kind of activity in an educational setting at scale. in this
work, we identified six key patterns in problem-solving behavior
among solvers of foldit. the protein folding challenges in foldit
present rich, completely open, heterogeneous solution spaces, making them a compelling domain in which to analyze these patterns.
to facilitate the identification of these patterns, we used an iterative
methodology to design visualizations of solvers’ problem-solving
activity as solution trees. the size and complexity of the foldit data
required us to develop domain-specific techniques to summarize the
solution trees and render them tractable for analysis while preserving the salient problem-solving behaviors. finally, we compared the



38

occurrence of the patterns we identified between high- and lowerperforming solvers. we found that high-performing solvers explore
more broadly and more aggressively avoid local optima. we also
found that both categories of solvers employ automation and manual
intervention in similar quantities, inviting future work to study how
these tools are used at a more fine-grained level.
we have only scratched the surface in our analysis of a subset of
foldit data. two integral aspects of the foldit environment are
not within the scope of this work: collaboration and expert feedback. we only considered solutions produced by individual solvers,
but foldit solver can also take solutions produced by others and
try and improve them. this collaborative framework may involve
specialization and unique solving strategies, and deserves careful
study. expert feedback comes into play for design puzzles, where
biochemists will select a small number of the solutions to try and
synthesize in the lab. experts will also impose additional constraints
on future design puzzles to try and guide solutions toward more
promising designs. the interaction of these channels for expert
feedback and problem-solving behavior is an important topic for
future research. also outside the scope of this work is how individual solvers change their problem-solving behavior over time. many
solvers have been participating in the foldit community for many
years, and studying how their behavior evolves could yield insights
into the acquisition of high-level problem-solving skills.
looking more broadly at the impact of this work, our methodology
and analysis can serve as a first step toward discovering the scaffolding necessary to develop high-level problem-solving skills. these
results could contribute to a hint generation system, where solvers
could be guided toward known effective strategies, or a meta-planner
component in foldit that could tailor the parameters of particular
puzzles to optimize the quality of the scientific results. in all of
these cases, this work contributes to the necessary foundational
understanding of the problem-solving behavior involved.

8.

acknowledgements

this work was supported by the national institutes of health grant
1uh2ca203780, rosettacommons, and amazon. this material
is based upon work supported by the national science foundation
under grant no. 1629879.

9.

[6]

[7]

[8]
[9]
[10]

[11]
[12]

[13]
[14]
[15]

[16]

references

[1] e. andersen, y.-e. liu, e. apter, f. boucher-genesse, and
z. popović. gameplay analysis through state projection. in
proceedings of the fifth international conference on the
foundations of digital games, pages 1–8. acm, 2010.
[2] s. cooper, f. khatib, i. makedon, h. lu, j. barbero, d. baker,
j. fogarty, z. popović, et al. analysis of social gameplay
macros in the foldit cookbook. in proceedings of the 6th
international conference on foundations of digital games,
pages 9–14. acm, 2011.
[3] s. cooper, f. khatib, a. treuille, j. barbero, j. lee,
m. beenen, a. leaver-fay, d. baker, z. popović, et al.
predicting protein structures with a multiplayer online game.
nature, 466(7307):756–760, 2010.
[4] m. eagle, d. hicks, b. peddycord iii, and t. barnes.
exploring networks of problem-solving interactions. in
proceedings of the 5th conference on learning analytics and
knowledge. acm, 2015.
[5] c. b. eiben, j. b. siegel, j. b. bale, s. cooper, f. khatib,

[17]

[18]
[19]
[20]

[21]

b. w. shen, b. l. stoddard, z. popovic, and d. baker.
increased diels-alderase activity through backbone
remodeling guided by foldit players. nature biotechnology,
30(2):190–192, 2012.
m. h. falakmasir, j. p. gonzalez-brenes, g. j. gordon, and
k. e. dicerbo. a data-driven approach for inferring student
proficiency from game activity logs. in proceedings of the
third (2016) acm conference on learning@ scale, pages
341–349. acm, 2016.
c. geigle, c. zhai, and d. c. ferguson. an exploration of
automated grading of complex assignments. in proceedings of
the third (2016) acm conference on learning@ scale, pages
351–360. acm, 2016.
m. l. gick. problem-solving strategies. educational
psychologist, 21(1-2):99–120, 1986.
j. g. greeno. natures of problem-solving abilities. handbook
of learning and cognitive processes, 5:239–270, 1978.
e. harpstead, c. j. maclellan, k. r. koedinger, v. aleven,
s. p. dow, and b. myers. investigating the solution space of
an open-ended educational game using conceptual feature
extraction. in proceedings of the 6th conference on
educational data mining, 2013.
w. hung, d. h. jonassen, r. liu, et al. problem-based
learning. handbook of research on educational
communications and technology, 3:485–506, 2008.
m. johnson, m. eagle, and t. barnes. invis: an interactive
visualization tool for exploring interaction networks. in
proceedings of the 6th conference on educational data
mining, 2013.
d. h. jonassen. toward a design theory of problem solving.
educational technology research and development,
48(4):63–85, dec 2000.
d. a. joyner. expert evaluation of 300 projects per day. in
proceedings of the third (2016) acm conference on
learning@ scale, pages 121–124. acm, 2016.
f. khatib, s. cooper, m. d. tyka, k. xu, i. makedon,
z. popović, and d. baker. algorithm discovery by protein
folding game players. proceedings of the national academy
of sciences, 108(47):18949–18953, 2011.
f. khatib, f. dimaio, s. cooper, m. kazmierczyk, m. gilski,
s. krzywda, h. zabranska, i. pichova, j. thompson,
z. popović, et al. crystal structure of a monomeric retroviral
protease solved by protein folding game players. nature
structural & molecular biology, 18(10):1175–1177, 2011.
l. malkiewich, r. s. baker, v. shute, s. kai, and l. paquette.
classifying behavior to elucidate elegant problem solving in
an educational game. in proceedings of the 9th conference on
educational data mining, 2016.
e. rowe, r. s. baker, and j. asbell-clarke. strategic game
moves mediate implicit science learning. in proceedings of
the 8th conference on educational data mining, 2015.
h. a. simon. information-processing theory of human
problem solving. handbook of learning and cognitive
processes, 5:271–295, 1978.
k. tóth, h. rölke, s. greiff, and s. wüstenberg. discovering
students’ complex problem solving strategies in educational
assessment. in proceedings of the 7th conference on
educational data mining, 2014.
g. wallner and s. kriglstein. visualization-based analysis of
gameplay data–a review of literature. entertainment
computing, 4(3):143–155, 2013.

